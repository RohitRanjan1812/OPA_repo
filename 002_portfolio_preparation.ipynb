{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34c17937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime ,timedelta\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "\n",
    "# for data read\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e75e677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual Inputs\n",
    "# We choose dates between 2010 and 2019 as this 10 year period has no major financial global crises\n",
    "# We intend to use 9 years (2010-2018) for training, and 1 year (2019) for testing\n",
    "\n",
    "#root_dir          = r'C:\\Users\\wsteynber\\_Data Science'\n",
    "root_dir          = r'c:\\Users\\Zoli\\Downloads\\python\\academy'\n",
    "price_metric      = 'Open'\n",
    "filt_date_min     = datetime(2010,1,1)\n",
    "filt_date_max     = datetime(2019,12,1) + MonthEnd(0) + timedelta(days=1)\n",
    "filt_date_min_ext = filt_date_min - timedelta(days=10) # extra days used for linear interpolation\n",
    "filt_date_max_ext = filt_date_max + timedelta(days=10) # extra days used for linear interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b85d764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import time series data and filter according to input dates\n",
    "\n",
    "df_ts_0_raw         = pd.read_pickle(root_dir + r'\\50yr_timeSeries_data.pkl')\n",
    "\n",
    "df_ts_0_raw         = df_ts_0_raw.loc[: ,(slice(None) ,[price_metric])]\n",
    "df_ts_0_raw.columns = df_ts_0_raw.columns.droplevel(1)\n",
    "\n",
    "filt                = ((filt_date_min_ext <= df_ts_0_raw.index) & (df_ts_0_raw.index < filt_date_max_ext))\n",
    "df_ts_0_raw         = df_ts_0_raw.loc[filt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5045583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2009-12-22', '2009-12-23', '2009-12-24', '2009-12-28',\n",
       "               '2009-12-29', '2009-12-30', '2009-12-31', '2010-01-04',\n",
       "               '2010-01-05', '2010-01-06', '2010-01-07', '2010-01-08',\n",
       "               '2010-01-11', '2010-01-12', '2010-01-13', '2010-01-14',\n",
       "               '2010-01-15', '2010-01-19', '2010-01-20', '2010-01-21'],\n",
       "              dtype='datetime64[ns]', name='Date', freq=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We observe no data on weekends and other days (possibly public holidays?)\n",
    "# ie 2010-01-01 to 2010-01-03 missing\n",
    "# ie 2010-01-09 to 2010-01-10 missing ... and so on\n",
    "\n",
    "df_ts_0_raw[:20].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c723ba0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRGP</th>\n",
       "      <th>FDS</th>\n",
       "      <th>SYY</th>\n",
       "      <th>RCL</th>\n",
       "      <th>PSA</th>\n",
       "      <th>FISV</th>\n",
       "      <th>HUM</th>\n",
       "      <th>SNA</th>\n",
       "      <th>EMR</th>\n",
       "      <th>CAT</th>\n",
       "      <th>...</th>\n",
       "      <th>INTC</th>\n",
       "      <th>KDP</th>\n",
       "      <th>ADP</th>\n",
       "      <th>WMB</th>\n",
       "      <th>CCL</th>\n",
       "      <th>ATVI</th>\n",
       "      <th>DLTR</th>\n",
       "      <th>DLR</th>\n",
       "      <th>FE</th>\n",
       "      <th>ODFL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>66.532499</td>\n",
       "      <td>28.327501</td>\n",
       "      <td>25.622500</td>\n",
       "      <td>82.622503</td>\n",
       "      <td>12.270625</td>\n",
       "      <td>44.475001</td>\n",
       "      <td>42.897501</td>\n",
       "      <td>43.187501</td>\n",
       "      <td>57.612499</td>\n",
       "      <td>...</td>\n",
       "      <td>20.647501</td>\n",
       "      <td>28.740000</td>\n",
       "      <td>38.219930</td>\n",
       "      <td>17.453432</td>\n",
       "      <td>32.192500</td>\n",
       "      <td>11.32</td>\n",
       "      <td>16.184166</td>\n",
       "      <td>50.110001</td>\n",
       "      <td>47.227500</td>\n",
       "      <td>9.162963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>66.424999</td>\n",
       "      <td>28.285001</td>\n",
       "      <td>25.795000</td>\n",
       "      <td>82.335003</td>\n",
       "      <td>12.266250</td>\n",
       "      <td>44.460001</td>\n",
       "      <td>42.845001</td>\n",
       "      <td>43.175001</td>\n",
       "      <td>57.625000</td>\n",
       "      <td>...</td>\n",
       "      <td>20.695001</td>\n",
       "      <td>28.670000</td>\n",
       "      <td>38.222124</td>\n",
       "      <td>17.461584</td>\n",
       "      <td>32.225000</td>\n",
       "      <td>11.29</td>\n",
       "      <td>16.181666</td>\n",
       "      <td>50.200001</td>\n",
       "      <td>47.025000</td>\n",
       "      <td>9.140741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>66.317499</td>\n",
       "      <td>28.242501</td>\n",
       "      <td>25.967500</td>\n",
       "      <td>82.047503</td>\n",
       "      <td>12.261875</td>\n",
       "      <td>44.445001</td>\n",
       "      <td>42.792501</td>\n",
       "      <td>43.162501</td>\n",
       "      <td>57.637501</td>\n",
       "      <td>...</td>\n",
       "      <td>20.742501</td>\n",
       "      <td>28.600000</td>\n",
       "      <td>38.224319</td>\n",
       "      <td>17.469736</td>\n",
       "      <td>32.257501</td>\n",
       "      <td>11.26</td>\n",
       "      <td>16.179166</td>\n",
       "      <td>50.290001</td>\n",
       "      <td>46.822499</td>\n",
       "      <td>9.118519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>66.209999</td>\n",
       "      <td>28.200001</td>\n",
       "      <td>26.139999</td>\n",
       "      <td>81.760002</td>\n",
       "      <td>12.257500</td>\n",
       "      <td>44.430000</td>\n",
       "      <td>42.740002</td>\n",
       "      <td>43.150002</td>\n",
       "      <td>57.650002</td>\n",
       "      <td>...</td>\n",
       "      <td>20.790001</td>\n",
       "      <td>28.530001</td>\n",
       "      <td>38.226513</td>\n",
       "      <td>17.477888</td>\n",
       "      <td>32.290001</td>\n",
       "      <td>11.23</td>\n",
       "      <td>16.176666</td>\n",
       "      <td>50.380001</td>\n",
       "      <td>46.619999</td>\n",
       "      <td>9.096296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>66.750000</td>\n",
       "      <td>28.010000</td>\n",
       "      <td>26.110001</td>\n",
       "      <td>79.870003</td>\n",
       "      <td>12.312500</td>\n",
       "      <td>45.790001</td>\n",
       "      <td>42.630001</td>\n",
       "      <td>43.380001</td>\n",
       "      <td>58.549999</td>\n",
       "      <td>...</td>\n",
       "      <td>20.940001</td>\n",
       "      <td>28.959999</td>\n",
       "      <td>37.445126</td>\n",
       "      <td>17.534952</td>\n",
       "      <td>32.279999</td>\n",
       "      <td>11.24</td>\n",
       "      <td>16.013332</td>\n",
       "      <td>50.029999</td>\n",
       "      <td>46.650002</td>\n",
       "      <td>8.642963</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 503 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            TRGP        FDS        SYY        RCL        PSA       FISV  \\\n",
       "Date                                                                      \n",
       "2010-01-01   NaN  66.532499  28.327501  25.622500  82.622503  12.270625   \n",
       "2010-01-02   NaN  66.424999  28.285001  25.795000  82.335003  12.266250   \n",
       "2010-01-03   NaN  66.317499  28.242501  25.967500  82.047503  12.261875   \n",
       "2010-01-04   NaN  66.209999  28.200001  26.139999  81.760002  12.257500   \n",
       "2010-01-05   NaN  66.750000  28.010000  26.110001  79.870003  12.312500   \n",
       "\n",
       "                  HUM        SNA        EMR        CAT  ...       INTC  \\\n",
       "Date                                                    ...              \n",
       "2010-01-01  44.475001  42.897501  43.187501  57.612499  ...  20.647501   \n",
       "2010-01-02  44.460001  42.845001  43.175001  57.625000  ...  20.695001   \n",
       "2010-01-03  44.445001  42.792501  43.162501  57.637501  ...  20.742501   \n",
       "2010-01-04  44.430000  42.740002  43.150002  57.650002  ...  20.790001   \n",
       "2010-01-05  45.790001  42.630001  43.380001  58.549999  ...  20.940001   \n",
       "\n",
       "                  KDP        ADP        WMB        CCL   ATVI       DLTR  \\\n",
       "Date                                                                       \n",
       "2010-01-01  28.740000  38.219930  17.453432  32.192500  11.32  16.184166   \n",
       "2010-01-02  28.670000  38.222124  17.461584  32.225000  11.29  16.181666   \n",
       "2010-01-03  28.600000  38.224319  17.469736  32.257501  11.26  16.179166   \n",
       "2010-01-04  28.530001  38.226513  17.477888  32.290001  11.23  16.176666   \n",
       "2010-01-05  28.959999  37.445126  17.534952  32.279999  11.24  16.013332   \n",
       "\n",
       "                  DLR         FE      ODFL  \n",
       "Date                                        \n",
       "2010-01-01  50.110001  47.227500  9.162963  \n",
       "2010-01-02  50.200001  47.025000  9.140741  \n",
       "2010-01-03  50.290001  46.822499  9.118519  \n",
       "2010-01-04  50.380001  46.619999  9.096296  \n",
       "2010-01-05  50.029999  46.650002  8.642963  \n",
       "\n",
       "[5 rows x 503 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DataFrame with every day of the week\n",
    "# Use linear interpolation to populate, where possible, missing values\n",
    "# Remove dates (indexes) that were just there for linear interpolation purposes\n",
    "\n",
    "df_all_dates = pd.DataFrame(pd.date_range(filt_date_min_ext ,filt_date_max_ext ,freq='d') ,columns=['Date']).set_index(['Date'])\n",
    "\n",
    "df_ts_1_lin_inter = pd.concat([df_all_dates ,df_ts_0_raw] ,axis=1)\n",
    "df_ts_1_lin_inter = df_ts_1_lin_inter.interpolate(limit_direction='forward')\n",
    "\n",
    "filt              = ((filt_date_min <= df_ts_1_lin_inter.index) & (df_ts_1_lin_inter.index < filt_date_max))\n",
    "df_ts_1_lin_inter = df_ts_1_lin_inter[filt]\n",
    "\n",
    "df_ts_1_lin_inter.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45b3ec96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Distribution of NaN values by Stock')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check NaN values in time series data\n",
    "# We observe that after linear interpolation only an extremely small percentage of stocks had NaN price values\n",
    "\n",
    "df_stocks_with_nan             = pd.DataFrame(df_ts_1_lin_inter.isna().sum() ,columns=['nan_cnt'])\n",
    "df_stocks_with_nan['nan_perc'] = df_stocks_with_nan['nan_cnt'] / len(df_ts_1_lin_inter)\n",
    "\n",
    "ax = df_stocks_with_nan['nan_perc'].plot.hist()\n",
    "ax.set_xlabel('NaN Percentage')\n",
    "ax.set_title('Distribution of NaN values by Stock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d58d68cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values remaining:  0\n"
     ]
    }
   ],
   "source": [
    "# Drop stocks with > 50% NaN price values as they contain very little information\n",
    "# For the remainder set NaN values equal to stock median\n",
    "\n",
    "filt = (df_stocks_with_nan['nan_perc'] > 0.5)\n",
    "lst_stocks_w_nan_to_drop = df_stocks_with_nan.loc[filt].index\n",
    "\n",
    "df_ts_2_nan_dropped = df_ts_1_lin_inter.drop(columns=lst_stocks_w_nan_to_drop ,axis=1).copy()\n",
    "df_ts_2_nan_dropped = df_ts_2_nan_dropped.fillna(df_ts_2_nan_dropped.median())\n",
    "\n",
    "print('NaN values remaining: ' ,max(df_ts_2_nan_dropped.isna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "207e78d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stocks in alternative portfolios but not in time series data: 23\n",
      "Of which are non-dropped stocks: 0\n"
     ]
    }
   ],
   "source": [
    "# Import dictionary of all the alternative portfolios\n",
    "# We observe that the only stocks in the alternative portfolios not in the time series data are the stocks dropped above (lst_stocks_w_nan_to_drop)\n",
    "\n",
    "with open(root_dir + r'\\alternate_port.pkl' ,'rb') as pickle_in:\n",
    "    dict_alt_port_0_raw = pickle.load(pickle_in)\n",
    "\n",
    "lst_alt_port_stocks = dict_alt_port_0_raw['cluster_0'].index.tolist() # cluster_0 is the first alternative portfolio and contains all stocks (see clustering for further detail)\n",
    "lst_df_ts_stocks    = df_ts_2_nan_dropped.columns.tolist()\n",
    "\n",
    "print('Number of stocks in alternative portfolios but not in time series data:' ,len(set(lst_alt_port_stocks) - set(lst_df_ts_stocks)))\n",
    "print('Of which are non-dropped stocks:'                                        ,len(set(lst_alt_port_stocks) - set(lst_df_ts_stocks) - set(lst_stocks_w_nan_to_drop)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2721264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove dropped stocks from all alternative portfolios\n",
    "\n",
    "dict_alt_port_1_nan_dropped = dict_alt_port_0_raw.copy()\n",
    "\n",
    "for i in range(len(dict_alt_port_1_nan_dropped)):\n",
    "    lst_tmp_keep_stocks                         =  list(set(dict_alt_port_1_nan_dropped[f'cluster_{i}'].index) - set(lst_stocks_w_nan_to_drop))\n",
    "    dict_alt_port_1_nan_dropped[f'cluster_{i}'] = dict_alt_port_1_nan_dropped[f'cluster_{i}'].loc[lst_tmp_keep_stocks]\n",
    "    # print('From' ,len(dict_alt_port_0_raw[f'cluster_{i}'].index) ,'to' ,len(dict_alt_port_1_nan_dropped[f'cluster_{i}'].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1170c682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove redundant columns from all alternative portfolios\n",
    "\n",
    "lst_drop_cols  = [f'cluster_{i}' for i in range(len(dict_alt_port_1_nan_dropped))] # These columns were used for clustering and are redundant for the time series piece\n",
    "lst_drop_cols += ['overall_score' ,'_rn']\n",
    "\n",
    "for i in range(len(dict_alt_port_1_nan_dropped)):\n",
    "    dict_alt_port_1_nan_dropped[f'cluster_{i}'] = dict_alt_port_1_nan_dropped[f'cluster_{i}'].drop(lst_drop_cols ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0adc8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt_port_13']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visual to check that the shrunken version of the alternative portfoios do not have too few stocks in them\n",
    "# We observe the alternative portfolio cluster_13 has nan values\n",
    "\n",
    "lst_nan_cols = []\n",
    "\n",
    "df_tmp_idx = pd.DataFrame(index=dict_alt_port_1_nan_dropped['cluster_0']['cluster'].drop_duplicates())\n",
    "\n",
    "for i in range(len(dict_alt_port_1_nan_dropped)):\n",
    "    df_tmp        = pd.DataFrame(dict_alt_port_1_nan_dropped[f'cluster_{i}']['cluster'].value_counts()).rename(columns={'cluster':f'alt_port_{i}'})\n",
    "    df_tmp        = pd.concat([df_tmp_idx ,df_tmp] ,axis=1)\n",
    "    lst_nan_cols += df_tmp.columns[df_tmp.isna().any()].tolist()\n",
    "\n",
    "lst_nan_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc51fc24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alt_port_12_pre</th>\n",
       "      <th>alt_port_12_post</th>\n",
       "      <th>alt_port_13_pre</th>\n",
       "      <th>alt_port_13_post</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         alt_port_12_pre  alt_port_12_post  alt_port_13_pre  alt_port_13_post\n",
       "cluster                                                                      \n",
       "23                     2                 1                1               NaN"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Investigate what can be done for the nan value in alternative portfolio cluster_13\n",
    "# We see there's only a single nan value in alternative portfolio 13 at cluster 23\n",
    "\n",
    "df_tmp = pd.DataFrame(index=dict_alt_port_1_nan_dropped['cluster_0']['cluster'].drop_duplicates())\n",
    "\n",
    "for i in range(12,14,1): #range(len(dict_alt_port_1_nan_dropped)):\n",
    "    df_tmp_frm = pd.DataFrame(dict_alt_port_0_raw        [f'cluster_{i}']['cluster'].value_counts()).rename(columns={'cluster':f'alt_port_{i}_pre'})\n",
    "    df_tmp_to  = pd.DataFrame(dict_alt_port_1_nan_dropped[f'cluster_{i}']['cluster'].value_counts()).rename(columns={'cluster':f'alt_port_{i}_post'})\n",
    "    df_tmp = pd.concat([df_tmp ,df_tmp_frm ,df_tmp_to] ,axis=1)\n",
    "\n",
    "df_tmp.index.name = 'cluster'\n",
    "df_tmp.loc[df_tmp['alt_port_13_post'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5089b226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem notes for cluster 23:\n",
    "#     From the above, cluster 23 has the nan value\n",
    "#     \"_pre\"  in alt_port_[#]_pre  means the number of stocks in the cluster prior to  the data cleaning (column drop) above\n",
    "#     \"_post\" in alt_port_[#]_post means the number of stocks in the cluster following the data cleaning (column drop) above\n",
    "\n",
    "# Problem explanation cluster 23:\n",
    "#     alt_port_12_pre  = 2 means the alternative portfolio had 2 stocks\n",
    "#     alt_port_12_post = 1 means one of these stocks were dropped as part of the data cleaning above\n",
    "#     alt_port_13_pre  = 1 means 1 stock was chosen out of the 2 in alt_port_12_pre\n",
    "#     alt_port_13_post = 0 means the stock in alt_port_13_pre was dropped\n",
    "#\n",
    "#     conclusion: of the 2 stocks in alt_port_12_pre 1 is not included in the dropped stocks list above (lst_stocks_w_nan_to_drop)\n",
    "\n",
    "# Solution for cluster 23:\n",
    "#    take the stock from alt_port_12_pre that wasn't dropped (ie not in alt_port_12_post)\n",
    "#    insert into alt_port_13_post\n",
    "\n",
    "filt   = (dict_alt_port_0_raw        ['cluster_12']['cluster'] == 23)\n",
    "filt_0 = (dict_alt_port_1_nan_dropped['cluster_12']['cluster'] == 23)\n",
    "\n",
    "lst_solution_stock = list(  set(dict_alt_port_0_raw['cluster_12'].loc[filt].index.tolist())\n",
    "                          - set(dict_alt_port_0_raw['cluster_13'].loc[filt].index.tolist()))\n",
    "lst_solution_stock\n",
    "\n",
    "df_solultion_stock = dict_alt_port_0_raw['cluster_12'].loc[filt].loc[lst_solution_stock]\n",
    "df_solultion_stock\n",
    "\n",
    "dict_alt_port_2_nan_clusters_fix               = dict_alt_port_1_nan_dropped.copy()\n",
    "dict_alt_port_2_nan_clusters_fix['cluster_13'] = pd.concat([dict_alt_port_2_nan_clusters_fix['cluster_13'] ,df_solultion_stock])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1b7e68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with nan values: []\n",
      "Is the stock used for the fix one that should be dropped: False\n"
     ]
    }
   ],
   "source": [
    "# Check that the fix worked and all clusters in the shrunked alternative portfolios have stocks in them\n",
    "# And double check that the stock used for the fix is not included in the list of stocks that were/should be dropped\n",
    "# We observe the fix worked\n",
    "\n",
    "lst_nan_cols = []\n",
    "\n",
    "df_tmp_idx = pd.DataFrame(index=dict_alt_port_2_nan_clusters_fix['cluster_0']['cluster'].drop_duplicates())\n",
    "\n",
    "for i in range(len(dict_alt_port_2_nan_clusters_fix)):\n",
    "    df_tmp = pd.DataFrame(dict_alt_port_2_nan_clusters_fix[f'cluster_{i}']['cluster'].value_counts()).rename(columns={'cluster':f'alt_port_{i}'})\n",
    "    df_tmp = pd.concat([df_tmp_idx ,df_tmp] ,axis=1)\n",
    "    \n",
    "    lst_nan_cols += df_tmp.columns[df_tmp.isna().any()].tolist()\n",
    "\n",
    "print('Columns with nan values:' ,lst_nan_cols)\n",
    "print('Is the stock used for the fix one that should be dropped:' ,lst_solution_stock in lst_stocks_w_nan_to_drop.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f76e6bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights sum to: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TRGP    0.002805\n",
       "FDS     0.002127\n",
       "SYY     0.002391\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate weights to apply to each stock over all alternative portfolios\n",
    "\n",
    "from sklearn.preprocessing  import MinMaxScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# # Calculate the daily returns of each stock\n",
    "returns = df_ts_2_nan_dropped.pct_change().dropna()\n",
    "\n",
    "# # Normalize the returns using a MinMaxScaler\n",
    "scaled_returns = MinMaxScaler().fit_transform(returns)\n",
    "scaled_returns = pd.DataFrame(scaled_returns ,index=returns.index ,columns=returns.columns)\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "train_size = int(len(scaled_returns) * 0.8)\n",
    "train_data ,test_data = scaled_returns[:train_size] ,scaled_returns[train_size:]\n",
    "\n",
    "# # Define the input and output data for the neural network\n",
    "X_train ,y_train = train_data[:-1] ,train_data[1:]\n",
    "X_test  ,y_test  = test_data [:-1] ,test_data [1:]\n",
    "\n",
    "# # Define and train the neural network\n",
    "model = MLPRegressor(hidden_layer_sizes=(32,16) ,activation='relu' ,solver='adam' ,random_state=42)\n",
    "model.fit(X_train ,y_train)\n",
    "\n",
    "# # Use the trained neural network to predict the optimal weights for the testing data\n",
    "df_mlp_weights = model.predict(X_test)[-1]\n",
    "df_mlp_weights = df_mlp_weights / np.sum(df_mlp_weights)\n",
    "df_mlp_weights = pd.Series(df_mlp_weights ,index=X_test.columns)\n",
    "\n",
    "# Ensure weights sum to 1\n",
    "print('Weights sum to:' ,np.sum(df_mlp_weights))\n",
    "df_mlp_weights.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "693ec3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP WEIGHTS\n",
      "TRGP    0.002805\n",
      "FDS     0.002127\n",
      "SYY     0.002391\n",
      "dtype: float64 \n",
      "\n",
      "<bound method DataFrame.round of                                    Date       TRGP        FDS        SYY\n",
      "INFO                                                                    \n",
      "Step 0 - Original price      2010-01-01  57.299559  66.532499  28.327501\n",
      "Step 1 - Apply mlp weighting 2010-01-01   0.160753   0.141503   0.067735\n",
      "Step 2 - Scaled              2010-01-01   0.434867   0.382791   0.183235\n",
      "Step 0 - Original price      2010-01-02  57.299559  66.424999  28.285001\n",
      "Step 1 - Apply mlp weighting 2010-01-02   0.160753   0.141274   0.067633\n",
      "Step 2 - Scaled              2010-01-02   0.434867   0.382173   0.182960\n",
      "Step 0 - Original price      2010-01-03  57.299559  66.317499  28.242501\n",
      "Step 1 - Apply mlp weighting 2010-01-03   0.160753   0.141046   0.067532\n",
      "Step 2 - Scaled              2010-01-03   0.434867   0.381554   0.182685> \n",
      "\n",
      "Check base/start day sums to 1: \n",
      " Date\n",
      "2010-01-01    1.000893\n",
      "2010-01-02    1.000000\n",
      "2010-01-03    0.999107\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Develop and test a method to dynamically apply weighting and scale values\n",
    "# Scale the data for simplicity assuming datetime(2010,1,2) is the base/start day\n",
    "\n",
    "lst_tmp_stocks = df_mlp_weights.index[:3].tolist()\n",
    "lst_tmp_stocks\n",
    "\n",
    "df_tmp_mlp_weights                       = df_mlp_weights.loc[lst_tmp_stocks]\n",
    "\n",
    "df_tmp_price_pre_weight                  = df_ts_2_nan_dropped[lst_tmp_stocks].iloc[:3]\n",
    "df_tmp_price_post_weight                 = df_tmp_price_pre_weight.transpose().multiply(df_tmp_mlp_weights ,axis='rows').transpose()\n",
    "\n",
    "filt = (df_tmp_price_post_weight.index == datetime(2010,1,2))\n",
    "df_tmp_price_post_scaled                = df_tmp_price_post_weight.div(df_tmp_price_post_weight.iloc[filt].sum(axis=1).sum() ,axis=0)\n",
    "\n",
    "df_tmp_price_pre_weight ['INFO'] = 'Step 0 - Original price'\n",
    "df_tmp_price_post_weight['INFO'] = 'Step 1 - Apply mlp weighting'\n",
    "df_tmp_price_post_scaled['INFO'] = 'Step 2 - Scaled'\n",
    "\n",
    "df_tmp_price_all = pd.concat([df_tmp_price_pre_weight ,df_tmp_price_post_weight ,df_tmp_price_post_scaled])\n",
    "df_tmp_price_all = df_tmp_price_all.reset_index().set_index('INFO').sort_values(['Date' ,'INFO'])\n",
    "\n",
    "print('MLP WEIGHTS')\n",
    "print(df_tmp_mlp_weights ,'\\n')\n",
    "\n",
    "print(df_tmp_price_all.round ,'\\n')\n",
    "\n",
    "filt = (df_tmp_price_all.index == 'Step 2 - Scaled')\n",
    "\n",
    "print('Check base/start day sums to 1: \\n' ,df_tmp_price_all.set_index('Date').loc[filt].sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "249578e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISRG</th>\n",
       "      <th>ACN</th>\n",
       "      <th>DE</th>\n",
       "      <th>AEE</th>\n",
       "      <th>NKE</th>\n",
       "      <th>SBAC</th>\n",
       "      <th>MA</th>\n",
       "      <th>ON</th>\n",
       "      <th>HOLX</th>\n",
       "      <th>PNR</th>\n",
       "      <th>...</th>\n",
       "      <th>ANET</th>\n",
       "      <th>ATVI</th>\n",
       "      <th>TFC</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>HES</th>\n",
       "      <th>HII</th>\n",
       "      <th>ED</th>\n",
       "      <th>XEL</th>\n",
       "      <th>UAL</th>\n",
       "      <th>COF</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>0.721765</td>\n",
       "      <td>0.767563</td>\n",
       "      <td>1.000645</td>\n",
       "      <td>0.432144</td>\n",
       "      <td>0.317315</td>\n",
       "      <td>0.934824</td>\n",
       "      <td>0.513764</td>\n",
       "      <td>0.155054</td>\n",
       "      <td>0.302706</td>\n",
       "      <td>0.327433</td>\n",
       "      <td>...</td>\n",
       "      <td>1.164006</td>\n",
       "      <td>0.232201</td>\n",
       "      <td>0.506370</td>\n",
       "      <td>0.210581</td>\n",
       "      <td>0.967546</td>\n",
       "      <td>4.524781</td>\n",
       "      <td>0.951193</td>\n",
       "      <td>0.363046</td>\n",
       "      <td>0.230101</td>\n",
       "      <td>0.765549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-02</th>\n",
       "      <td>0.721507</td>\n",
       "      <td>0.766826</td>\n",
       "      <td>0.998872</td>\n",
       "      <td>0.430130</td>\n",
       "      <td>0.317135</td>\n",
       "      <td>0.934145</td>\n",
       "      <td>0.513446</td>\n",
       "      <td>0.155318</td>\n",
       "      <td>0.302913</td>\n",
       "      <td>0.326630</td>\n",
       "      <td>...</td>\n",
       "      <td>1.164006</td>\n",
       "      <td>0.231585</td>\n",
       "      <td>0.507361</td>\n",
       "      <td>0.210766</td>\n",
       "      <td>0.969554</td>\n",
       "      <td>4.524781</td>\n",
       "      <td>0.948206</td>\n",
       "      <td>0.362750</td>\n",
       "      <td>0.230816</td>\n",
       "      <td>0.767043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-03</th>\n",
       "      <td>0.721249</td>\n",
       "      <td>0.766088</td>\n",
       "      <td>0.997100</td>\n",
       "      <td>0.428116</td>\n",
       "      <td>0.316956</td>\n",
       "      <td>0.933466</td>\n",
       "      <td>0.513128</td>\n",
       "      <td>0.155581</td>\n",
       "      <td>0.303119</td>\n",
       "      <td>0.325827</td>\n",
       "      <td>...</td>\n",
       "      <td>1.164006</td>\n",
       "      <td>0.230970</td>\n",
       "      <td>0.508352</td>\n",
       "      <td>0.210951</td>\n",
       "      <td>0.971561</td>\n",
       "      <td>4.524781</td>\n",
       "      <td>0.945218</td>\n",
       "      <td>0.362453</td>\n",
       "      <td>0.231532</td>\n",
       "      <td>0.768537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>0.720990</td>\n",
       "      <td>0.765351</td>\n",
       "      <td>0.995327</td>\n",
       "      <td>0.426102</td>\n",
       "      <td>0.316776</td>\n",
       "      <td>0.932787</td>\n",
       "      <td>0.512810</td>\n",
       "      <td>0.155845</td>\n",
       "      <td>0.303326</td>\n",
       "      <td>0.325025</td>\n",
       "      <td>...</td>\n",
       "      <td>1.164006</td>\n",
       "      <td>0.230354</td>\n",
       "      <td>0.509344</td>\n",
       "      <td>0.211136</td>\n",
       "      <td>0.973568</td>\n",
       "      <td>4.524781</td>\n",
       "      <td>0.942230</td>\n",
       "      <td>0.362157</td>\n",
       "      <td>0.232247</td>\n",
       "      <td>0.770031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>0.722891</td>\n",
       "      <td>0.776042</td>\n",
       "      <td>1.021692</td>\n",
       "      <td>0.422149</td>\n",
       "      <td>0.312364</td>\n",
       "      <td>0.962387</td>\n",
       "      <td>0.512512</td>\n",
       "      <td>0.156725</td>\n",
       "      <td>0.308694</td>\n",
       "      <td>0.333253</td>\n",
       "      <td>...</td>\n",
       "      <td>1.164006</td>\n",
       "      <td>0.230560</td>\n",
       "      <td>0.509740</td>\n",
       "      <td>0.211214</td>\n",
       "      <td>1.001276</td>\n",
       "      <td>4.524781</td>\n",
       "      <td>0.937285</td>\n",
       "      <td>0.354873</td>\n",
       "      <td>0.230101</td>\n",
       "      <td>0.789351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 480 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ISRG       ACN        DE       AEE       NKE      SBAC  \\\n",
       "Date                                                                     \n",
       "2010-01-01  0.721765  0.767563  1.000645  0.432144  0.317315  0.934824   \n",
       "2010-01-02  0.721507  0.766826  0.998872  0.430130  0.317135  0.934145   \n",
       "2010-01-03  0.721249  0.766088  0.997100  0.428116  0.316956  0.933466   \n",
       "2010-01-04  0.720990  0.765351  0.995327  0.426102  0.316776  0.932787   \n",
       "2010-01-05  0.722891  0.776042  1.021692  0.422149  0.312364  0.962387   \n",
       "\n",
       "                  MA        ON      HOLX       PNR  ...      ANET      ATVI  \\\n",
       "Date                                                ...                       \n",
       "2010-01-01  0.513764  0.155054  0.302706  0.327433  ...  1.164006  0.232201   \n",
       "2010-01-02  0.513446  0.155318  0.302913  0.326630  ...  1.164006  0.231585   \n",
       "2010-01-03  0.513128  0.155581  0.303119  0.325827  ...  1.164006  0.230970   \n",
       "2010-01-04  0.512810  0.155845  0.303326  0.325025  ...  1.164006  0.230354   \n",
       "2010-01-05  0.512512  0.156725  0.308694  0.333253  ...  1.164006  0.230560   \n",
       "\n",
       "                 TFC      GOOG       HES       HII        ED       XEL  \\\n",
       "Date                                                                     \n",
       "2010-01-01  0.506370  0.210581  0.967546  4.524781  0.951193  0.363046   \n",
       "2010-01-02  0.507361  0.210766  0.969554  4.524781  0.948206  0.362750   \n",
       "2010-01-03  0.508352  0.210951  0.971561  4.524781  0.945218  0.362453   \n",
       "2010-01-04  0.509344  0.211136  0.973568  4.524781  0.942230  0.362157   \n",
       "2010-01-05  0.509740  0.211214  1.001276  4.524781  0.937285  0.354873   \n",
       "\n",
       "                 UAL       COF  \n",
       "Date                            \n",
       "2010-01-01  0.230101  0.765549  \n",
       "2010-01-02  0.230816  0.767043  \n",
       "2010-01-03  0.231532  0.768537  \n",
       "2010-01-04  0.232247  0.770031  \n",
       "2010-01-05  0.230101  0.789351  \n",
       "\n",
       "[5 rows x 480 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary of DataFrames with time series data containing the market portfolio and the alternative portfolios\n",
    "# Scale the data for simplicity assuming 1 Jan 2019, datetime(2019,1,1), is the base/start day\n",
    "# ie Values for each stock at 1 Jan 2019 represent the $ figure to invest in that stock assuming you have $1000 to create that specific alternative portfolio\n",
    "\n",
    "dict_ts_alt_port = dict()\n",
    "base_date        = datetime(2019,1,1)\n",
    "base_amount      = 1000\n",
    "\n",
    "filt = (df_ts_2_nan_dropped.index == base_date)\n",
    "df_ts_tmp_market_port = df_ts_2_nan_dropped.multiply(base_amount).div(df_ts_2_nan_dropped.iloc[filt].sum(axis=1).sum() ,axis=0)\n",
    "\n",
    "dict_ts_alt_port['market_port'] = df_ts_tmp_market_port\n",
    "\n",
    "for i in range(len(dict_alt_port_2_nan_clusters_fix)):\n",
    "\n",
    "    lst_tmp_stocks = dict_alt_port_2_nan_clusters_fix[f'cluster_{i}'].index.tolist()\n",
    "\n",
    "    df_tmp_mlp_weights                       = df_mlp_weights.loc[lst_tmp_stocks]\n",
    "\n",
    "    df_tmp_price_pre_weight                  = df_ts_2_nan_dropped[lst_tmp_stocks]\n",
    "    df_tmp_price_post_weight                 = df_tmp_price_pre_weight.transpose().multiply(df_tmp_mlp_weights ,axis='rows').transpose()\n",
    "\n",
    "    filt = (df_tmp_price_post_weight.index == base_date)\n",
    "    df_tmp_price_post_scaled                = df_tmp_price_post_weight.multiply(base_amount).div(df_tmp_price_post_weight.iloc[filt].sum(axis=1).sum() ,axis=0)\n",
    "\n",
    "    dict_ts_alt_port[f'alt_port_{i}']       = df_tmp_price_post_scaled\n",
    "\n",
    "dict_ts_alt_port['alt_port_0'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54345e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(root_dir + r'\\dict_ts_alt_port.pkl' ,'wb') as handle:\n",
    "    pickle.dump(dict_ts_alt_port ,handle ,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
